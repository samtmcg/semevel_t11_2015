{\rtf1\ansi\ansicpg1252\cocoartf1138\cocoasubrtf510
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\qc

\f0\b\fs24 \cf0 Task 11 Test Data  README
\b0 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural
\cf0 \
The test data for Task 11 comprises 4000 tweet ids. These tweets were chosen to be representative examples of different categories of figurative/non-figurative content. \
\
The tweets contain metaphors, ironic statements, sarcastic statements, as well as general statements (which may contain any or none of the preceding phenomena). This latter category, named Other, comprises 30% of the data, and was chosen to be a general sampling of Twitter content. The rump of the test data, comprising 70% of the tweets overall, was chosen to exhibit either sarcasm, irony or metaphor. \
\
The tweet ids are sorted in the test data, and no inference should be drawn from how the tweet ids cluster in the data.\
\
Your task, or that of your system, is to assign an integer sentiment score in the range -5 to +5 to each of these tweet ids. Your system's sentiment ratings will be compared to the gold standard aggregated from human annotators on the CrowdFlower crowd-sourcing platform.\
\
Note that the human annotators were not informed as to the category of the tweets they were annotating. That is, they were not told whether a given tweet was meant to be ironic, sarcastic, metaphoric or other. They evaluated each tweet based wholly on its textual content  (including its hashtags) and nothing more.\
\
Please see the official Task 11 page for details regarding the Scorer and its operation:\
\
{\field{\*\fldinst{HYPERLINK "http://alt.qcri.org/semeval2015/task11/"}}{\fldrslt http://alt.qcri.org/semeval2015/task11/}}\
\
Good luck!\
\
\
\
}